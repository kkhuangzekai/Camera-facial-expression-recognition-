{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpGHrA9LY2TF",
        "outputId": "2944dda8-a408-4152-e234-530f7dc1d008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Expression'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://gitee.com/kjj0518/Expression.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKsJO2e6Y7GK",
        "outputId": "d501e60e-9adb-4658-d0b0-1f4a1a812036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Expression/models\n"
          ]
        }
      ],
      "source": [
        "cd Expression/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhIdu4DPZB3D",
        "outputId": "0bae9eca-eb8a-434d-af65-94c272014fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wR7kCZH7cPlkA_XhTZqMNC3u2i8T50Oh\n",
            "To: /content/Expression/models/model.pth\n",
            "100% 114M/114M [00:01<00:00, 97.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1wR7kCZH7cPlkA_XhTZqMNC3u2i8T50Oh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3yDQYtkZM6A",
        "outputId": "da92dd59-d0d2-4a6e-843f-0576054b7e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Expression\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF4H-AoLaWU7",
        "outputId": "04be12cd-dc0f-4d12-c3f1-2110478255fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch==0.6\n",
            "  Downloading efficientnet_pytorch-0.6.0.tar.gz (16 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch==0.6) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch==0.6) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.0-py3-none-any.whl size=12411 sha256=ab01acfa67e2ee73ba1ea6fbbd153b57c85aebbc319c3f56fe1eeb9232a007eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/3e/35/e54695cf32698610843bb6629152efd7b558bb6472ff4b0bdd\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch==0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZpjniYZZOxB",
        "outputId": "7ccfa9f2-e608-478f-9550-6f8bf746ec92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "priors nums:17640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.padding.ZeroPad2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saliency_map size : torch.Size([3, 3])\n",
            "saliency_map size : torch.Size([3, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import torch\n",
        "from vision.ssd.config.fd_config import define_img_size\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import io\n",
        "import os\n",
        "import PIL\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from utils import visualize_cam, Normalize\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "from torchvision.transforms import ToPILImage\n",
        "parser = argparse.ArgumentParser(description='detect_video')\n",
        "\n",
        "parser.add_argument('--input_size', default=640, type=int,\n",
        "                    help='define network input size,default optional value 128/160/320/480/640/1280')\n",
        "parser.add_argument('--threshold', default=0.7, type=float,\n",
        "                    help='score threshold')\n",
        "parser.add_argument('--candidate_size', default=1000, type=int,\n",
        "                    help='nms candidate size')\n",
        "parser.add_argument('--path', default=\"imgs\", type=str,\n",
        "                    help='imgs dir')\n",
        "parser.add_argument('--test_device', default=\"cuda:0\", type=str,\n",
        "                    help='cuda:0 or cpu')\n",
        "parser.add_argument('--video_path', default=\".MP4\", type=str,\n",
        "                    help='path of video')\n",
        "args = parser.parse_known_args()[0]\n",
        "\n",
        "input_img_size = args.input_size\n",
        "define_img_size(input_img_size)\n",
        "transform =transforms.Compose([transforms.Resize([100, 100]),transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "from vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\n",
        "from vision.utils.misc import Timer\n",
        "\n",
        "# cap = cv2.VideoCapture(args.video_path)  # capture from video\n",
        "\n",
        "class_id={'Surprise':0,'Fear':1,'Disgust':2,'Happy':3,'Sad':4,'Angry':5,'Neutral':6}\n",
        "express=list(class_id)\n",
        "test_device = args.test_device\n",
        "candidate_size = args.candidate_size\n",
        "threshold = args.threshold\n",
        "\n",
        "\n",
        "if input_img_size>= 640 :\n",
        "    model_path = \"models/face-detect-640.pth\"\n",
        "else:\n",
        "    model_path = \"models/face-detect-320.pth\"\n",
        "net = create_Mb_Tiny_RFB_fd(2, is_test=True, device=test_device)\n",
        "predictor = create_Mb_Tiny_RFB_fd_predictor(net, candidate_size=candidate_size, device=test_device)\n",
        "plt.ion()\n",
        "net.load(model_path)\n",
        "ExpressionNet=torch.load('models/model.pth').to(test_device)\n",
        "ExpressionNet.eval()\n",
        "timer = Timer()\n",
        "sum = 0\n",
        "\n",
        "ind = 0.1+0.6*np.arange(len(express))    # the x locations for the groups\n",
        "width = 0.4       # the width of the bars: can also be len(x) sequence\n",
        "color_list = ['darkgreen','darkviolet','deepskyblue','limegreen','blue','red','gold']\n",
        "\n",
        "cam_dict = dict()\n",
        "\n",
        "efficientnet_model_dict = dict(type='efficientnet', arch=ExpressionNet, layer_name='_conv_head', input_size=(100, 100))\n",
        "efficientnet_gradcam = GradCAM(efficientnet_model_dict, True)\n",
        "efficientnet_gradcampp = GradCAMpp(efficientnet_model_dict, True)\n",
        "cam_dict['efficientnet'] = [efficientnet_gradcam, efficientnet_gradcampp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N6sZV5rloIdr"
      },
      "outputs": [],
      "source": [
        "def VideoCapture():\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      //div_out.style.backgroundColor='green'\n",
        "      \n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "\n",
        "      div_right = document.createElement('div');\n",
        "      div_out.appendChild(div_right);\n",
        "      //div_right.style.backgroundColor='yellow'\n",
        "\n",
        "      img1 = document.createElement('img');\n",
        "      div_right.appendChild(img1);\n",
        "      img2 = document.createElement('img');\n",
        "      div_right.appendChild(img2);\n",
        "      img3 = document.createElement('img');\n",
        "      div_right.appendChild(img3);\n",
        "      img4 = document.createElement('img');\n",
        "      div_right.appendChild(img4);\n",
        "      \n",
        "      \n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64,num){\n",
        "      if (num==0){img.src = \"data:image/jpg;base64,\" + imgb64;}\n",
        "      if (num==1){img1.src = \"data:image/jpg;base64,\" + imgb64;}\n",
        "      if (num==2){img2.src = \"data:image/jpg;base64,\" + imgb64;}\n",
        "      if (num==3){img3.src = \"data:image/jpg;base64,\" + imgb64;}\n",
        "      if (num==4){img4.src = \"data:image/jpg;base64,\" + imgb64;}\n",
        "    }\n",
        "    \n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "def image2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'jpeg')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x\n",
        "\n",
        "def png2byte(image):\n",
        "  image = Image.fromarray(image)\n",
        "  buffer = io.BytesIO()\n",
        "  image.save(buffer, 'png')\n",
        "  buffer.seek(0)\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from google.colab.patches import cv2_imshow\n",
        "# classify_result = []\n",
        "# cam = []\n",
        "# canvas = plt.figure().canvas\n",
        "# VideoCapture()\n",
        "# eval_js('create()')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oqbyBPC8xXjb",
        "outputId": "b590712e-4c98-4788-a855-951d8a13ac64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      //div_out.style.backgroundColor='green'\n",
              "      \n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "\n",
              "      div_right = document.createElement('div');\n",
              "      div_out.appendChild(div_right);\n",
              "      //div_right.style.backgroundColor='yellow'\n",
              "\n",
              "      img1 = document.createElement('img');\n",
              "      div_right.appendChild(img1);\n",
              "      img2 = document.createElement('img');\n",
              "      div_right.appendChild(img2);\n",
              "      img3 = document.createElement('img');\n",
              "      div_right.appendChild(img3);\n",
              "      img4 = document.createElement('img');\n",
              "      div_right.appendChild(img4);\n",
              "      \n",
              "      \n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64,num){\n",
              "      if (num==0){img.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==1){img1.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==2){img2.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==3){img3.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==4){img4.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "    }\n",
              "    \n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3PYhbnfcTV1Z",
        "outputId": "409dee0d-81ef-4f9e-cd3a-fbffd4f297f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      //div_out.style.backgroundColor='green'\n",
              "      \n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "\n",
              "      div_right = document.createElement('div');\n",
              "      div_out.appendChild(div_right);\n",
              "      //div_right.style.backgroundColor='yellow'\n",
              "\n",
              "      img1 = document.createElement('img');\n",
              "      div_right.appendChild(img1);\n",
              "      img2 = document.createElement('img');\n",
              "      div_right.appendChild(img2);\n",
              "      img3 = document.createElement('img');\n",
              "      div_right.appendChild(img3);\n",
              "      img4 = document.createElement('img');\n",
              "      div_right.appendChild(img4);\n",
              "      \n",
              "      \n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64,num){\n",
              "      if (num==0){img.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==1){img1.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==2){img2.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==3){img3.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "      if (num==4){img4.src = \"data:image/jpg;base64,\" + imgb64;}\n",
              "    }\n",
              "    \n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time:  0.011228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time:  0.010193\n",
            "Inference time:  0.011057\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from google.colab.patches import cv2_imshow\n",
        "classify_result = []\n",
        "cam = []\n",
        "canvas = plt.figure().canvas\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "while True:\n",
        "    classify_result.clear()\n",
        "    cam.clear()\n",
        "    byte = eval_js('capture()')\n",
        "    image = byte2image(byte)\n",
        "    boxes, labels, probs = predictor.predict(image, candidate_size / 2, threshold)    \n",
        "    for i in range(0,boxes.size(0),1):\n",
        "        box = boxes[i, :]\n",
        "        x = box[0] + box[2]\n",
        "        x = int(x / 2)\n",
        "        y = box[1] + box[3]\n",
        "        y = int(y / 2)\n",
        "        w =box[2]-box[0]+box[3]-box[1]\n",
        "        w=w.int().item()//4\n",
        "        face = image[max(y-w,0):y+w,x-w:x+w]\n",
        "        face = Image.fromarray(face)\n",
        "        face = transform(face)\n",
        "        out = ExpressionNet(face.unsqueeze(0).to(test_device))\n",
        "        expression = torch.argmax(out).item()\n",
        "        score = torch.nn.functional.softmax(out,dim=1).squeeze()\n",
        "        cv2.rectangle(image, (x-w,y-w), (x+w, y+w), (0, 255, 0), 4)\n",
        "        cv2.putText(image, express[expression],\n",
        "                    (box[0], box[1] - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1,  # font scale\n",
        "                    (255, 0, 0),\n",
        "                    2)  # line type\n",
        "        for gradcam, gradcam_pp in cam_dict.values():\n",
        "          mask_pp, _ = gradcam_pp(face.unsqueeze(0).to(test_device))\n",
        "          heatmap_pp, result_pp = visualize_cam(mask_pp.cpu(), face.unsqueeze(0)*0.5+0.5)\n",
        "        cam.append(np.asarray(ToPILImage()(result_pp).resize((256,256))))\n",
        "        plt.clf()\n",
        "        plt.title(\"Classification results \", fontsize=15)\n",
        "        plt.xlabel(\" Expression Category \", fontsize=12)\n",
        "        plt.ylabel(\" Classification Score \", fontsize=12)\n",
        "        plt.xticks(ind, express, rotation=15, fontsize=16)\n",
        "        plt.ylim([0,1])\n",
        "        for i in range(len(express)):\n",
        "            plt.bar(ind[i], score.data.cpu().numpy()[i], width, color=color_list[i])\n",
        "        buffer = io.BytesIO()\n",
        "        canvas.print_png(buffer)\n",
        "        data = buffer.getvalue()\n",
        "        buffer.write(data)\n",
        "        buffer.seek(0)\n",
        "        classify_result.append(b64encode(buffer.read()).decode('utf-8'))\n",
        "    eval_js('showimg(\"{0}\",0)'.format(image2byte(image)))\n",
        "    for i in range(len(classify_result)):\n",
        "      eval_js('showimg(\"{0}\",\"{1}\")'.format(classify_result[i],i*2+1))\n",
        "      eval_js('showimg(\"{0}\",\"{1}\")'.format(image2byte(cam[i]),i*2+2))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "摄像头表情识别.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}